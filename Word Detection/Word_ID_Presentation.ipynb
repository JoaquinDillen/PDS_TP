{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4984dd1b",
   "metadata": {},
   "source": [
    "# Phonem Feature Extraction Detection \n",
    "\n",
    "This code have the purpose of perfomer phonem detection out of a voice recorder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hunga_bunga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c495bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f76e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import math\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "import os\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import sin, cos, pi, linspace, arange, log10, absolute\n",
    "from numpy.random import randn\n",
    "from scipy.signal import lfilter, lfilter_zi, filtfilt, butter, freqz, welch\n",
    "from scipy.io.wavfile import read , write\n",
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ee698",
   "metadata": {},
   "source": [
    "# Load audio file and present their information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load one specific audio\n",
    "\n",
    "filename = 'A_lata_01.wav'\n",
    "directory = 'C:/Users/Dasil/1. Processamento Digital do Sinal/Project/word Detection/training/' + filename\n",
    "\n",
    "fs, audio1 = read(directory)\n",
    "f,t,S1 = spectrogram(audio1, fs, window='flattop', nperseg=fs//10, noverlap=fs//20, scaling='spectrum', mode='magnitude')\n",
    "\n",
    "#print information about the audio\n",
    "print('filename: ', filename)\n",
    "print('Data Length (s): ',t[-1])\n",
    "print('Sampling frequency (samples/s): ', fs)\n",
    "\n",
    "#Playing Audio (Reproduce Audio)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5428921",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92250801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cepstrum and Power Spectrum\n",
    "\n",
    "def real_cepstrum(x, n=None):\n",
    "# Compute the real cepstrum of a real sequence\n",
    "#    x : ndarray\n",
    "#        Real sequence to compute real cepstrum of.\n",
    "#    n : {None, int}, optional\n",
    "#        Length of the Fourier transform.\n",
    "#    Returns\n",
    "#    -------\n",
    "#    ceps: ndarray\n",
    "#        The real cepstrum.\n",
    "\n",
    "    spectrum = np.abs(np.fft.fft(x, n=n))**2\n",
    "    ceps = np.fft.ifft(np.log(spectrum))\n",
    "\n",
    "    return ceps, spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b2509bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero Crossing Rate\n",
    "\n",
    "def ZCR(samples, frameSize, overlap):\n",
    "    wlen = len(samples)\n",
    "    step = frameSize - overlap\n",
    "    frameNum = math.ceil(wlen/step)\n",
    "    zcr = np.zeros((frameNum,1))\n",
    "    for i in range(frameNum):\n",
    "        curFrame = samples[np.arange(i*step,min(i*step+frameSize,wlen))]\n",
    "        #To avoid DC bias, usually we need to perform mean substraction on each frame\n",
    "        curFrame = curFrame - np.mean(curFrame) #Zero-Justified\n",
    "        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]<=0)\n",
    "    return zcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23033152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro ButterWord\n",
    "\n",
    "wn = 1500/(fs/2)\n",
    "b3,a3 = butter(4, wn)\n",
    "#Apply the filter\n",
    "audio1_filt = lfilter(b3,a3,audio1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41fa2b",
   "metadata": {},
   "source": [
    "# First Audio Segmentation\n",
    "\n",
    "Separate the word from the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e239d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply first threshold\n",
    "results = [1 if item > 500 else 0 for item in audio1_filt]\n",
    "\n",
    "# Variables\n",
    "x1 = x2 = 0\n",
    "flag = False\n",
    "chunks = []\n",
    "coordinates = []\n",
    "zeros_list = []\n",
    "silence_len = 10000\n",
    "min_len = 10000\n",
    "\n",
    "plt.plot(results)\n",
    "for i, result in enumerate(results):\n",
    "    # First signal rise\n",
    "    if(flag == False and result == 1 and x1 == 0):\n",
    "        flag = True\n",
    "        x1 = i\n",
    "    # Check if fall is permanent\n",
    "    if(flag == True):\n",
    "        if(result == 0):\n",
    "            zeros_list.append(result)\n",
    "        else:\n",
    "            zeros_list = []\n",
    "    # Signal fall\n",
    "    if(flag == True and len(zeros_list) >= silence_len and x2 == 0):\n",
    "        flag = False\n",
    "        x2 = i - len(zeros_list)\n",
    "        # If data is valid, save it in list\n",
    "        if(not x2-x1 <= min_len):\n",
    "            chunks.append(results[x1:x2+1])\n",
    "            coordinates.append([x1, x2])\n",
    "            plt.plot([x1, x1], [0, 1], [x2, x2], [0, 1], marker='o')\n",
    "        # Reset to initial state\n",
    "        x1 = x2 = 0\n",
    "\n",
    "print(f'# of chunks detected: {len(chunks)}')\n",
    "\n",
    "pt1 = coordinates[0]\n",
    "try:\n",
    "    pt2 = coordinates[1]\n",
    "    div_ptn = ((pt2[0] - pt1[1]) / 2) + pt1[1]\n",
    "except:\n",
    "    div_ptn = ((pt1[1] + pt1[0]) / 2)\n",
    "    \n",
    "# Draw dividing line\n",
    "plt.plot([div_ptn, div_ptn], [0, 1], marker='o', color=\"black\")\n",
    "print(pt1[0], pt1[1])\n",
    "print(pt1[1] - pt1[0])\n",
    "\n",
    "#Make segmentation point integer\n",
    "div_ptn = round(div_ptn)\n",
    "\n",
    "# Fix-sized segmentation (breaks a signal into non-overlapping segments)\n",
    "signal = audio1 / (2**15)\n",
    "signal_len = len(signal)\n",
    "segment_size_t = 1 # segment size in seconds\n",
    "segment_size = segment_size_t * fs  # segment size in samples\n",
    "\n",
    "# Break signal into list of segments in a single-line Python code\n",
    "segment1 = audio1[:div_ptn]\n",
    "segment2 = audio1[div_ptn:]\n",
    "segments = [segment1, segment2]\n",
    "\n",
    "# Find out the number of Segments\n",
    "n = len(segments)\n",
    "\n",
    "# Acess the first chunk of the audio\n",
    "samples = segments[0]\n",
    "samples = np.array(samples)\n",
    "\n",
    "# Process each chunk\n",
    "# Save each segment in a seperate filename\n",
    "for iS, s in enumerate(segments):\n",
    "    write('Word Detection/PD_data/'+filename+'_{0:d}.wav'.format(segment_size_t * iS, segment_size_t * (iS + 1)), fs, (s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d76ac6",
   "metadata": {},
   "source": [
    "# Word Selection for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbba9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load one specific word\n",
    "\n",
    "filename = 'A_lata_01'\n",
    "directory = 'C:/Users/Dasil/1. Processamento Digital do Sinal/Project/word Detection/training/' + filename + '_1.wav'\n",
    "\n",
    "fs, word = read(directory)\n",
    "f,t,S1 = spectrogram(word, fs, window='flattop', nperseg=fs//10, noverlap=fs//20, scaling='spectrum', mode='magnitude')\n",
    "\n",
    "#print information about the word\n",
    "print('filename: ', filename)\n",
    "print('Data Length (s): ',t[-1])\n",
    "print('Sampling frequency (samples/s): ', fs)\n",
    "\n",
    "#Playing word (Reproduce word)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fba9b",
   "metadata": {},
   "source": [
    "# Spectrogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f546b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic Spectrogram \n",
    "\n",
    "plt.rcParams['figure.figsize'] = 14,5\n",
    "plt.pcolormesh(t, f[:450], S1[:450][:])\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel('time(s)')\n",
    "plt.ylabel('frequency(Hz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro ButterWord\n",
    "\n",
    "wn = 1500/(fs/2)\n",
    "b3,a3 = butter(4, wn)\n",
    "#Apply the filter\n",
    "word_filt = lfilter(b3,a3,word)\n",
    "\n",
    "plt.plot(word_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a57373",
   "metadata": {},
   "source": [
    "# Show Word Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot word Filtered and only positive part\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 16,5\n",
    "#plot(audio1,'r')\n",
    "plt.plot(np.absolute(word_filt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbef5a1",
   "metadata": {},
   "source": [
    "# Segment Audio\n",
    "\n",
    "Do the first segmentation in order to isolate the desired word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17916c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Threshold for the word phonems\n",
    "\n",
    "results = [1 if item > 0.001*1e8 else 0 for item in audio1_filt**2]\n",
    "plt.plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "x1 = x2 = 0\n",
    "flag = False\n",
    "chunks = []\n",
    "coordinates = []\n",
    "zeros_list = []\n",
    "silence_len = 2000\n",
    "min_len = 5000\n",
    "\n",
    "plt.plot(results)\n",
    "for i, result in enumerate(results):\n",
    "    # First signal rise\n",
    "    if(flag == False and result == 1 and x1 == 0):\n",
    "        flag = True\n",
    "        x1 = i\n",
    "    # Check if fall is permanent\n",
    "    if(flag == True):\n",
    "        if(result == 0):\n",
    "            zeros_list.append(result)\n",
    "        else:\n",
    "            zeros_list = []\n",
    "    # Signal fall\n",
    "    if(flag == True and len(zeros_list) >= silence_len and x2 == 0):\n",
    "        flag = False\n",
    "        x2 = i - len(zeros_list)\n",
    "        # If data is valid, save it in list\n",
    "        if(not x2-x1 <= min_len):\n",
    "            chunks.append(results[x1:x2+1])\n",
    "            coordinates.append([x1, x2])\n",
    "            plt.plot([x1, x1], [0, 1], [x2, x2], [0, 1], marker='o')\n",
    "        # Reset to initial state\n",
    "        x1 = x2 = 0\n",
    "\n",
    "print(f'# of chunks detected: {len(chunks)}')\n",
    "\n",
    "pt1 = coordinates[0]\n",
    "try:\n",
    "    pt2 = coordinates[1]\n",
    "    div_ptn = ((pt2[0] - pt1[1]) / 2) + pt1[1]\n",
    "except:\n",
    "    div_ptn = ((pt1[1] + pt1[0]) / 2)\n",
    "    \n",
    "# Draw dividing line\n",
    "plt.plot([div_ptn, div_ptn], [0, 1], marker='o', color=\"black\")\n",
    "print(pt1[0], pt1[1])\n",
    "print(pt1[1] - pt1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ebe53",
   "metadata": {},
   "source": [
    "# Split Word\n",
    "\n",
    "Split the word in two phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make segmentation point integer\n",
    "div_ptn = round(div_ptn)\n",
    "\n",
    "# Fix-sized segmentation (breaks a signal into non-overlapping segments)\n",
    "signal = audio1 / (2**15)\n",
    "signal_len = len(signal)\n",
    "segment_size_t = 1 # segment size in seconds\n",
    "segment_size = segment_size_t * fs  # segment size in samples\n",
    "\n",
    "# Break signal into list of segments in a single-line Python code\n",
    "segment1 = audio1[:div_ptn]\n",
    "segment2 = audio1[div_ptn:]\n",
    "segments = [segment1, segment2]\n",
    "\n",
    "# Find out the number of Segments\n",
    "n = len(segments)\n",
    "\n",
    "# Acess the first chunk of the audio\n",
    "samples = segments[0]\n",
    "samples = np.array(samples)\n",
    "\n",
    "# Process each chunk\n",
    "# Save each segment in a seperate filename\n",
    "for iS, s in enumerate(segments):\n",
    "    write('Word Detection/WD_data/'+filename+'_{0:d}.wav'.format(segment_size_t * iS, segment_size_t * (iS + 1)), fs, (s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a51c5",
   "metadata": {},
   "source": [
    "# MFCC's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c750fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = segments[0] #First phoneme \n",
    "\n",
    "#Play Audio\n",
    "ipd.Audio(filename)\n",
    "\n",
    "#Calculate MFCC'S\n",
    "mfccs = librosa.feature.mfcc(signal, sr=fs, n_mfcc=42, n_fft = 1024, hop_length = 50,\n",
    "                                n_mels = 1000,  fmin = 10, fmax = 4000)\n",
    "\n",
    "#Normalize MFCC's Results (Y axis)\n",
    "mfccs = sklearn.preprocessing.scale(mfccs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize MFCC \n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "sr = fs\n",
    "librosa.display.specshow(mfccs[1:],x_axis='time',sr=sr)\n",
    "plt.colorbar(format=\"%+2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54d79c",
   "metadata": {},
   "source": [
    "# Training and Classificator\n",
    "\n",
    "## Read and calculate MFCC for all segmented phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f435314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfccs(signal, fs):\n",
    "    \n",
    "    #Calculate MFCCs\n",
    "    mfccs = librosa.feature.mfcc(signal, sr=fs, n_mfcc=200, n_fft = 1024, hop_length = 50, n_mels = 1000,  fmin = 10, fmax = 4000)\n",
    "    \n",
    "    # Calculate average\n",
    "    #mfccs = np.average(mfccs, axis=1)\n",
    "    mfccs = np.average(np.average(mfccs, axis=1))\n",
    "    # Make it into a one-dimensional array\n",
    "    mfccs = mfccs.flatten()\n",
    "    mfccs = mfccs.tolist()\n",
    "    return mfccs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = 'C:/Users/lemos/PDS/TP/Word Detection/WD_data/'\n",
    "directory = 'C:/Users/Dasil/1. Processamento Digital do Sinal/Project/Word Detection/WD_data/'\n",
    "\n",
    "hope = True\n",
    "mfcc_data = []\n",
    "phoneme_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    \n",
    "    # Get audio file\n",
    "    filename = os.path.join(directory, file)\n",
    "    \n",
    "    # load Audio for Signal\n",
    "    signal, fs = librosa.load(filename)\n",
    "    \n",
    "    # Get mfccs from audio\n",
    "    mfccs = get_mfccs(signal, fs)\n",
    "    mfccs.insert(1,mfccs[0])\n",
    "    #mfccs = mfccs[:18]\n",
    "\n",
    "    if(hope):\n",
    "        \n",
    "        #Calculate Cepstrung and Power Spectrum\n",
    "        ceps, spec = real_cepstrum(signal, n=None)\n",
    "        N = signal.shape[0]\n",
    "\n",
    "        #Power Specturm\n",
    "        power_spec = np.abs(spec[:N//2])**2\n",
    "        index = np.where(power_spec == np.max(power_spec))\n",
    "        mfccs.append(index[0][0])\n",
    "        #mfccs.insert(1, mfccs[0])\n",
    "\n",
    "        #Calculate Zero Cross Rate\n",
    "        frameSize = 256\n",
    "        overlap = 0\n",
    "        zcr = ZCR(signal, frameSize, overlap)\n",
    "        mfccs.append(np.max(zcr))\n",
    "\n",
    "        #Calculate Cepstrum\n",
    "        abs_ceps = np.abs(ceps[:N//2])**2\n",
    "        mfccs.append(round(np.max(abs_ceps)))\n",
    "    else:\n",
    "        # Remove the first coeficient\n",
    "        mfccs.pop(0)\n",
    "\n",
    "        # Delete the features of some mfcc coeficients because they are not needed.\n",
    "        #mfccs = mfccs[:12]\n",
    "        #mfccs = mfccs[25:]\n",
    "        #mfccs = mfccs[:18]\n",
    "    \n",
    "    # Insert label\n",
    "    file = file.replace('.wav', '')\n",
    "    file_parts = file.split('_')\n",
    "    word = file_parts[1]\n",
    "    if (len(word) > 4):\n",
    "        if(file_parts[-1] == '0'):\n",
    "            phoneme = word[0:3]\n",
    "        else:\n",
    "            phoneme = word[3:]\n",
    "    else:\n",
    "        if(file_parts[-1] == '0'):\n",
    "            phoneme = word[0:2]\n",
    "        else:\n",
    "            phoneme = word[2:]\n",
    "    phoneme_list.append(phoneme)\n",
    "    mfccs.insert(0, phoneme)\n",
    "    mfcc_data.append(mfccs) \n",
    "\n",
    "#Calculate Cepstrung and Power Spectrum\n",
    "ceps, spec = real_cepstrum(signal, n=None)\n",
    "N = signal.shape[0]\n",
    "\n",
    "#Power Specturm\n",
    "power_spec = np.abs(spec[:N//2])**2\n",
    "mfccs = power_spec\n",
    "    \n",
    "    \n",
    "# Remove duplicates\n",
    "phoneme_list = list(dict.fromkeys(phoneme_list))\n",
    "print(f'Phonemes: {phoneme_list}')\n",
    "print(f'# of phonemes: {len(phoneme_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c973bf6",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8be3878e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.001454</td>\n",
       "      <td>391</td>\n",
       "      <td>77.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>-9.812004</td>\n",
       "      <td>456</td>\n",
       "      <td>68.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.047333</td>\n",
       "      <td>326</td>\n",
       "      <td>70.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-7.520754</td>\n",
       "      <td>447</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.977703</td>\n",
       "      <td>845</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>6</td>\n",
       "      <td>-7.281468</td>\n",
       "      <td>438</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>7</td>\n",
       "      <td>-7.363895</td>\n",
       "      <td>232</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>9</td>\n",
       "      <td>-7.302172</td>\n",
       "      <td>478</td>\n",
       "      <td>102.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>7</td>\n",
       "      <td>-7.458501</td>\n",
       "      <td>246</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>9</td>\n",
       "      <td>-7.203053</td>\n",
       "      <td>591</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1    2      3   4\n",
       "0    0 -7.001454  391   77.0   6\n",
       "1    6 -9.812004  456   68.0  61\n",
       "2    0 -7.047333  326   70.0   4\n",
       "3    6 -7.520754  447   43.0  10\n",
       "4    0 -7.977703  845  152.0   2\n",
       "..  ..       ...  ...    ...  ..\n",
       "223  6 -7.281468  438   34.0   8\n",
       "224  7 -7.363895  232  111.0   2\n",
       "225  9 -7.302172  478  102.0   4\n",
       "226  7 -7.458501  246  105.0   2\n",
       "227  9 -7.203053  591   54.0   4\n",
       "\n",
       "[228 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset loading\n",
    "df = pd.DataFrame(mfcc_data)\n",
    "\n",
    "x = df.iloc[:,1:] # MFCC features\n",
    "y = df.iloc[:,0] # Phoneme label\n",
    "\n",
    "# Label changed to number once\n",
    "label = set(y)\n",
    "label_list = list(label)\n",
    "label_list.sort()\n",
    "for i in range(len(label_list)):\n",
    "    y[y == label_list[i]] = i\n",
    "y = np.array(y, dtype = \"int\")\n",
    "\n",
    "# Preview dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806428d",
   "metadata": {},
   "source": [
    "# Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02414a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (159, 4)\n",
      "Test: (69, 4)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train result</th>\n",
       "      <th>Test result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>33.33</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poly</th>\n",
       "      <td>34.59</td>\n",
       "      <td>30.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBF</th>\n",
       "      <td>38.36</td>\n",
       "      <td>36.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>52.20</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Train result  Test result\n",
       "Linear         33.33        37.68\n",
       "Poly           34.59        30.43\n",
       "RBF            38.36        36.23\n",
       "KNN            52.20        33.33"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the train data and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
    "print(f'Train: {x_train.shape}\\nTest: {x_test.shape}\\n')\n",
    "\n",
    "# Data standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "# Instantiate an SVM\n",
    "model_linear = SVC(kernel='linear', random_state=1)\n",
    "model_poly = SVC(kernel='poly', random_state= 1)\n",
    "model_rbf = SVC(kernel='rbf', random_state=1)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# Fit models\n",
    "model_linear.fit(x_train_std, y_train)\n",
    "model_poly.fit(x_train_std, y_train)\n",
    "model_rbf.fit(x_train_std, y_train)\n",
    "model_knn.fit(x_train_std, y_train)\n",
    "\n",
    "# ===== Train data =====\n",
    "# Predictions\n",
    "pred_linear_train = model_linear.predict(x_train_std)\n",
    "pred_poly_train = model_poly.predict(x_train_std)\n",
    "pred_rbf_train = model_rbf.predict(x_train_std)\n",
    "pred_knn_train = model_knn.predict(x_train_std)\n",
    "# Accuracy scores\n",
    "accuracy_linear_train = accuracy_score(y_train, pred_linear_train)\n",
    "accuracy_poly_train = accuracy_score(y_train, pred_poly_train)\n",
    "accuracy_rbf_train = accuracy_score(y_train, pred_rbf_train)\n",
    "accuracy_knn_train = accuracy_score(y_train, pred_knn_train)\n",
    "\n",
    "#print('Train result')\n",
    "#print(f'Linear : {int(accuracy_linear_train*100)}%')\n",
    "#print(f'Poly : {int(accuracy_poly_train*100)}%')\n",
    "#print(f'RBF : {int(accuracy_rbf_train*100)}%')\n",
    "#print(\"-\" * 15)\n",
    "\n",
    "# ===== Test data =====\n",
    "# Predictions\n",
    "pred_linear_test = model_linear.predict(x_test_std)\n",
    "pred_poly_test = model_poly.predict(x_test_std)\n",
    "pred_rbf_test = model_rbf.predict(x_test_std)\n",
    "pred_knn_test = model_knn.predict(x_test_std)\n",
    "# Accuracy scores\n",
    "accuracy_linear_test = accuracy_score(y_test, pred_linear_test)\n",
    "accuracy_poly_test = accuracy_score(y_test, pred_poly_test)\n",
    "accuracy_rbf_test = accuracy_score(y_test, pred_rbf_test)\n",
    "accuracy_knn_test = accuracy_score(y_test, pred_knn_test)\n",
    "\n",
    "#print('Test result')\n",
    "#print(f'Linear : {int(accuracy_linear_test*100)}%')\n",
    "#print(f'Poly : {int(accuracy_poly_test*100)}%')\n",
    "#print(f'RBF : {int(accuracy_rbf_test*100)}%')\n",
    "\n",
    "# Create dataframe\n",
    "data = {'Train result':[accuracy_linear_train*100, accuracy_poly_train*100, accuracy_rbf_train*100, accuracy_knn_train*100],\n",
    "        'Test result': [accuracy_linear_test*100, accuracy_poly_test*100, accuracy_rbf_test*100, accuracy_knn_test*100]}\n",
    "df_results = pd.DataFrame(data)\n",
    "# Change the row indexes\n",
    "df_results.index = ['Linear', 'Poly', 'RBF', 'KNN']\n",
    "df_results.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6318049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear</th>\n",
       "      <th>Poly</th>\n",
       "      <th>RBF</th>\n",
       "      <th>KNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Linear Poly RBF KNN\n",
       "0     ca   ca  ca  ca"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'C:/Users/Dasil/1. Processamento Digital do Sinal/Project/word Detection/WD_data/A_chuta_02_0.wav' \n",
    "\n",
    "# load Audio for Signal\n",
    "signal, fs = librosa.load(filename)\n",
    "\n",
    "#Calculate MFCCs\n",
    "mfccs = get_mfccs(signal, fs)\n",
    "\n",
    "#mfccs.pop(0)\n",
    "\n",
    "#mfccs = mfccs[:18]\n",
    "\n",
    "#Calculate Cepstrung and Power Spectrum\n",
    "ceps, spec = real_cepstrum(signal, n=None)\n",
    "N = signal.shape[0]\n",
    "\n",
    "#Power Specturm\n",
    "power_spec = np.abs(spec[:N//2])**2\n",
    "index = np.where(power_spec == np.max(power_spec))\n",
    "mfccs.append(index[0][0])\n",
    "\n",
    "#Calculate Zero Cross Rate\n",
    "frameSize = 256\n",
    "overlap = 0\n",
    "zcr = ZCR(signal, frameSize, overlap)\n",
    "mfccs.append(np.max(zcr))\n",
    "\n",
    "#Calculate Cepstrum\n",
    "abs_ceps = np.abs(ceps[:N//2])**2\n",
    "mfccs.append(round(np.max(abs_ceps)))\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(mfccs).T\n",
    "#df\n",
    "\n",
    "#Data standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(df)\n",
    "x_std = sc.transform(df)\n",
    "\n",
    "pred_linear_test = model_linear.predict(x_std)\n",
    "pred_poly_test = model_poly.predict(x_std)\n",
    "pred_rbf_test = model_rbf.predict(x_std)\n",
    "pred_knn_test = model_knn.predict(x_std)\n",
    "\n",
    "data = {'Linear':[phoneme_list[int(pred_linear_test)]],\n",
    "        'Poly':[phoneme_list[int(pred_poly_test)]],\n",
    "        'RBF': [phoneme_list[int(pred_rbf_test)]],\n",
    "        'KNN': [phoneme_list[int(pred_knn_test)]]}\n",
    "\n",
    "df_models = pd.DataFrame(data) \n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(df)\n",
    "x_std = sc.transform(df)\n",
    "\n",
    "pred_linear_test = model_linear.predict(x_std)\n",
    "pred_poly_test = model_poly.predict(x_std)\n",
    "pred_rbf_test = model_rbf.predict(x_std)\n",
    "pred_knn_test = model_knn.predict(x_std)\n",
    "\n",
    "\n",
    "data = {'Linear':[phoneme_list[int(pred_linear_test)]],\n",
    "        'Poly':[phoneme_list[int(pred_poly_test)]],\n",
    "        'RBF': [phoneme_list[int(pred_rbf_test)]],\n",
    "        'KNN': [phoneme_list[int(pred_knn_test)]]}\n",
    "\n",
    "df_models = pd.DataFrame(data) \n",
    "df_models\n",
    "#print(phoneme_list[int(pred_linear_test)])\n",
    "#print(phoneme_list[int(pred_poly_test)])\n",
    "#print(phoneme_list[int(pred_rbf_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05980544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "models = {\n",
    "        'LogisticRegression'     : LogisticRegression(),\n",
    "        'SVM'                    : SVC(),\n",
    "        'RandomForestClassifier' : RandomForestClassifier(),\n",
    "        'KNN'                    : KNeighborsClassifier()}\n",
    "\n",
    "hyper = {\n",
    "        \n",
    "        'LogisticRegression':{\n",
    "                                    'penalty'     : ['l2'],\n",
    "                                    'C'           : np.logspace(0, 4, 10),\n",
    "                                    'solver'      : ['lbfgs', 'liblinear', 'saga'],\n",
    "                                    'class_weight': ['balanced'],\n",
    "                                    'random_state': [0]},\n",
    "        \n",
    "        'SVM':{\n",
    "                                    'C'           : [0.01, 0.1, 1, 10, 100, 1000],\n",
    "                                    'gamma'       : [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                                    'kernel'      : ['rbf', 'linear'],\n",
    "                                    'class_weight': ['balanced'],\n",
    "                                    'random_state': [0]},\n",
    "        \n",
    "        'RandomForestClassifier':{\n",
    "                                    'max_depth': [2, 3, 4],\n",
    "                                    'max_features': [2, 3, 4, 'auto', 'sqrt'],\n",
    "                                    'n_estimators': [10, 100, 500, 1000],\n",
    "                                    'class_weight': ['balanced'],\n",
    "                                    'random_state': [0]},\n",
    "       \n",
    "        'KNN':{\n",
    "                                    'n_neighbors': [5, 10, 15, 20],\n",
    "                                    'weights': ['uniform', 'distance']}\n",
    "                              \n",
    "                              \n",
    "    }\n",
    "\n",
    "\n",
    "for model_name in models.keys():\n",
    "\n",
    "    # Model selection\n",
    "    clf    = models[model_name]\n",
    "    params = hyper[model_name]\n",
    "\n",
    "    # Pipeline (standarization + classifier)\n",
    "    pipe = Pipeline([ ( 'scaler', StandardScaler() ), ( 'clf', clf ) ])\n",
    "\n",
    "    # Gridsearch cross-validation\n",
    "    grid = GridSearchCV(estimator = clf, param_grid = params, cv = 5, return_train_score = True)\n",
    "    grid.fit(x_train, y_train)\n",
    "\n",
    "    # Gridsearch cross-validation results\n",
    "    best_param                  = grid.best_params_\n",
    "    best_param_test_score_mean  = grid.cv_results_['mean_test_score'][grid.best_index_]\n",
    "    best_param_test_score_std   = grid.cv_results_['std_test_score'][grid.best_index_]\n",
    "    best_param_train_score_mean = grid.cv_results_['mean_train_score'][grid.best_index_]\n",
    "    best_param_train_score_std  = grid.cv_results_['std_train_score'][grid.best_index_]\n",
    "    \n",
    "print(best_param)\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf580ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf2da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hunga_bunga import HungaBungaClassifier, HungaBungaRegressor\n",
    "\n",
    "clf = HungaBungaClassifier()\n",
    "clf.fit(x,y)\n",
    "clf.prefit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b60b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
