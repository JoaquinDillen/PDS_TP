{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800f2c2f",
   "metadata": {},
   "source": [
    "# Pitch Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pyAudioAnalysis\n",
    "import pydub\n",
    "import ffmpeg\n",
    "\n",
    "from numpy import sin, cos, pi, linspace, arange, log10, absolute\n",
    "from numpy.random import randn\n",
    "from scipy.signal import lfilter, lfilter_zi, filtfilt, butter, freqz, welch\n",
    "from scipy.io.wavfile import read , write\n",
    "from scipy.signal import spectrogram\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pyplot import ylabel, xlabel\n",
    "from matplotlib.pyplot import plot, legend, show, grid, figure, savefig#, hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'audio/training/J_cata_02.wav'\n",
    "fs, audio1 = read(filename)\n",
    "f,t,S1 = spectrogram(audio1, fs, window='flattop', nperseg=fs//10, noverlap=fs//20, scaling='spectrum', mode='magnitude')\n",
    "print('filename: ', filename)\n",
    "print('Data Length (s): ',t[-1])\n",
    "print('Sampling frequency (samples/s): ', fs)\n",
    "\n",
    "#Playing Audio (Reproduce Audio)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec285f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectogram\n",
    "pyplot.rcParams['figure.figsize'] = 14,5\n",
    "pyplot.pcolormesh(t, f[:450], S1[:450][:])\n",
    "pyplot.title(\"Spectrogram\")\n",
    "xlabel('time(s)')\n",
    "ylabel('frequency(Hz)')\n",
    "#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro ButterWord\n",
    "\n",
    "wn = 1500/(fs/2)\n",
    "b3,a3 = butter(4, wn)\n",
    "audio1_filt = lfilter(b3,a3,audio1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2aa57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.rcParams['figure.figsize'] = 16,5\n",
    "#plot(audio1,'r')\n",
    "plot(absolute(audio1_filt),'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b46453",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#result = [1 if item > 0.0004*1e8 else 0 for item in audio1_filt**2]\n",
    "#plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be6b95",
   "metadata": {},
   "source": [
    "# Fix-sized segmentation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the first part we showed how we can segment a long recording to non-overlapping segments using ffmpeg. The following code sample shows how to do the same with Python. Line 8 does the actual segmentation in a single-line command. Overall, the following script loads and normalizes an audio signal, and then it breaks it into 1-second segments and writes each one of them in a file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Pay attention to the note in the last comment: you will need to cast to 16bit before saving to file because the numpy conversion has led to higher sample resolutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix-sized segmentation (breaks a signal into non-overlapping segments)\n",
    "    #signal = audio1 / (2**15)\n",
    "    #signal_len = len(signal)\n",
    "    #segment_size_t = 1 # segment size in seconds\n",
    "    #segment_size = segment_size_t * fs  # segment size in samples\n",
    "\n",
    "# Break signal into list of segments in a single-line Python code\n",
    "    #segments = np.array([signal[x:x + segment_size] for x in\n",
    "    #                     np.arange(0, signal_len, segment_size)])\n",
    "\n",
    "# Save each segment in a seperate filename\n",
    "    #for iS, s in enumerate(segments):\n",
    "    #    wavfile.write(\"data/obama_segment_{0:d}_{1:d}.wav\".format(segment_size_t * iS,\n",
    "    #                                                              segment_size_t * (iS + 1)), fs, (s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd860d2c",
   "metadata": {},
   "source": [
    "# Audio Division "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "song = AudioSegment.from_wav(filename)\n",
    "\n",
    "db = -30\n",
    "segments = split_on_silence(song, min_silence_len = 100, silence_thresh = db)\n",
    "while len(segments) != 2:\n",
    "    db = db-1\n",
    "    segments = split_on_silence(song, min_silence_len = 100, silence_thresh = db)\n",
    "\n",
    "    \n",
    "n = len(segments)\n",
    "print(\"Size of List\", n)\n",
    "\n",
    "\n",
    "# Process each chunk with your parameters\n",
    "for i, segment in enumerate(segments):\n",
    "\n",
    "    # Export the audio chunk with new bitrate.\n",
    "    print(\"Exporting chunk{0}.wav.\".format(i))\n",
    "    segment.export(\n",
    "        \"audio/data/chunk{0}.wav\".format(i),\n",
    "        bitrate = \"192k\",\n",
    "        format = \"wav\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb350f11",
   "metadata": {},
   "source": [
    "\n",
    "# Pitch Detection \n",
    "\n",
    "    Using  the cepstrum coeficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Cepstrum and Power Spectrum\n",
    "\n",
    "def real_cepstrum(x, n=None):\n",
    "# Compute the real cepstrum of a real sequence\n",
    "#    x : ndarray\n",
    "#        Real sequence to compute real cepstrum of.\n",
    "#    n : {None, int}, optional\n",
    "#        Length of the Fourier transform.\n",
    "#    Returns\n",
    "#    -------\n",
    "#    ceps: ndarray\n",
    "#        The real cepstrum.\n",
    "\n",
    "    spectrum = np.abs(np.fft.fft(x, n=n))**2\n",
    "    ceps = np.fft.ifft(np.log(spectrum))\n",
    "\n",
    "    return ceps, spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero Crossing Rate\n",
    "\n",
    "def ZCR(samples, frameSize, overlap):\n",
    "    wlen = len(samples)\n",
    "    step = frameSize - overlap\n",
    "    frameNum = math.ceil(wlen/step)\n",
    "    zcr = np.zeros((frameNum,1))\n",
    "    for i in range(frameNum):\n",
    "        curFrame = samples[np.arange(i*step,min(i*step+frameSize,wlen))]\n",
    "        #To avoid DC bias, usually we need to perform mean substraction on each frame\n",
    "        curFrame = curFrame - np.mean(curFrame) #Zero-Justified\n",
    "        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]<=0)\n",
    "    return zcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = segments[0].get_array_of_samples()\n",
    "samples = np.array(samples)\n",
    "\n",
    "#id_filename = 'audio/data/chunk0.wav'\n",
    "#fs, id_audio = read(id_filename)\n",
    "\n",
    "#Calculate Cepstrung and Power Spectrum\n",
    "ceps, spec = real_cepstrum(samples, n=None)\n",
    "N = samples.shape[0]\n",
    "\n",
    "#Calculate Zero Cross Rate\n",
    "frameSize = 256\n",
    "overlap = 0\n",
    "zcr = ZCR(samples, frameSize, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551699d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic the samples \n",
    "plt.rcParams['figure.figsize'] = 16,5\n",
    "plot(zcr,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e196a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for x in zcr:\n",
    "    a+= x\n",
    "    \n",
    "b = round(a[0])\n",
    "\n",
    "print(round(np.average(zcr)))\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Power Specturm\n",
    "power_spec = np.abs(spec[:N//2])**2\n",
    "plt.plot(power_spec[0:3000])\n",
    "\n",
    "#Zero Crossing Rate \n",
    "#zero_crosses_rate = np.nonzero(np.diff(samples > 0))[0]\n",
    "#print(zero_crosses_rate)\n",
    "\n",
    "index = np.where(power_spec == np.max(power_spec))\n",
    "print('max value index: ', index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_ceps = np.abs(ceps[:N//2])**2\n",
    "#plt.plot(abs_ceps[:10])\n",
    "np.max(abs_ceps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Power Spectrum: \", index[0], \"\\n\")\n",
    "print(\"Power Cepstrum: \", round(np.max(abs_ceps)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
